{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 詞頻矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [\"How to format my hard disk\", \n",
    "           \"Hard disk format problems \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how', 'to', 'format', 'my', 'hard', 'disk']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words0 = content[0].lower().split()\n",
    "words0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hard', 'disk', 'format', 'problems']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words1 = content[1].lower().split()\n",
    "words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(words0) | set(words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy \n",
    "m = numpy.zeros((2,7))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "for idx, w in enumerate(words):\n",
    "    #print(idx, w)\n",
    "    word_to_id[w]   = idx\n",
    "    id_to_word[idx] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'how': 0, 'to': 1, 'my': 2, 'disk': 3, 'hard': 4, 'problems': 5, 'format': 6}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'how', 1: 'to', 2: 'my', 3: 'disk', 4: 'hard', 5: 'problems', 6: 'format'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words0:\n",
    "    m[0,word_to_id[w]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'how': 0, 'to': 1, 'my': 2, 'disk': 3, 'hard': 4, 'problems': 5, 'format': 6}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words1:\n",
    "    m[1,word_to_id[w]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 0., 1.],\n",
       "       [0., 0., 0., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 7)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt(((m[0] - m[1]) ** 2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 sklearn 的 CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [\"How to format my hard disk\", \n",
    "           \"Hard disk format problems \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "# fit: 產生結構\n",
    "# transform: 轉換\n",
    "# fit_transform: 產生結構並轉換\n",
    "X= vectorizer.fit_transform(content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x7 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disk', 'format', 'hard', 'how', 'my', 'problems', 'to']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用相似度檢索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = ['This is a toy post about machine learning. Actually, it contains not much interesting stuff.',\n",
    "'Imaging databases can get huge.',\n",
    "'Most imaging databases safe images permanently.',\n",
    "'Imaging databases store images.',\n",
    "'Imaging databases store images. Imaging databases store images. Imaging databases store images.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x24 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 33 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'actually', 'can', 'contains', 'databases', 'get', 'huge', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'safe', 'store', 'stuff', 'this', 'toy']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "        1, 1],\n",
       "       [0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 3, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,\n",
       "        0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = 'imaging database'\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x24 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1-v2\n",
    "    return sp.linalg.norm(delta.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.872983346207417"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(((X[0] - new_post_vec).toarray() ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.872983346207417"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_raw(X[0], new_post_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff. 3.872983346207417\n",
      "Imaging databases can get huge. 2.0\n",
      "Most imaging databases safe images permanently. 2.23606797749979\n",
      "Imaging databases store images. 1.7320508075688772\n",
      "Imaging databases store images. Imaging databases store images. Imaging databases store images. 5.5677643628300215\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    d = dist_raw(X[i], new_post_vec)\n",
    "    print(contents[i], d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用相對距離"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(v1, v2):\n",
    "    v1_normalized  = v1 / sp.linalg.norm(v1.toarray()) \n",
    "    v2_normalized  = v2 / sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff. 1.414213562373095\n",
      "Imaging databases can get huge. 1.0514622242382672\n",
      "Most imaging databases safe images permanently. 1.0878894332937856\n",
      "Imaging databases store images. 1.0\n",
      "Imaging databases store images. Imaging databases store images. Imaging databases store images. 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    d = dist(X[i], new_post_vec)\n",
    "    print(contents[i], d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x15 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 24 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = 'imaging database'\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actually', 'contains', 'databases', 'huge', 'images', 'imaging', 'interesting', 'learning', 'machine', 'permanently', 'post', 'safe', 'store', 'stuff', 'toy']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff. 1.4142135623730951\n",
      "Imaging databases can get huge. 0.9194016867619662\n",
      "Most imaging databases safe images permanently. 1.0514622242382672\n",
      "Imaging databases store images. 1.0\n",
      "Imaging databases store images. Imaging databases store images. Imaging databases store images. 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    d = dist(X[i], new_post_vec)\n",
    "    print(contents[i], d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graphic'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.stem\n",
    "s = nltk.stem.SnowballStemmer('english')\n",
    "s.stem('graphics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imag\n",
      "imag\n",
      "imagin\n",
      "imagin\n"
     ]
    }
   ],
   "source": [
    "print(s.stem(\"imaging\"))\n",
    "print(s.stem(\"image\"))\n",
    "print(s.stem(\"imagination\"))\n",
    "print(s.stem(\"imagine\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = StemmedCountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actual', 'contain', 'databas', 'huge', 'imag', 'interest', 'learn', 'machin', 'perman', 'post', 'safe', 'store', 'stuff', 'toy']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x14 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 21 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = 'imaging database'\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x14 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff. 1.414213562373095\n",
      "Imaging databases can get huge. 0.6058108930553725\n",
      "Most imaging databases safe images permanently. 0.6296288974669553\n",
      "Imaging databases store images. 0.5176380902050415\n",
      "Imaging databases store images. Imaging databases store images. Imaging databases store images. 0.5176380902050415\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    d = dist(X[i], new_post_vec)\n",
    "    print(contents[i], d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = 'imaging database'\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff. 1.4142135623730951\n",
      "Imaging databases can get huge. 1.0577078978082235\n",
      "Most imaging databases safe images permanently. 1.1654660300609625\n",
      "Imaging databases store images. 1.0697577475609328\n",
      "Imaging databases store images. Imaging databases store images. Imaging databases store images. 1.0697577475609328\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    d = dist(X[i], new_post_vec)\n",
    "    print(contents[i], d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inheritence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dog(object):\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name \n",
    "        \n",
    "    def eat(self):\n",
    "        return 'I am eating'\n",
    "    \n",
    "    def woof(self):\n",
    "        return 'won won'\n",
    "\n",
    "class Cat(object):\n",
    "    def eat(self):\n",
    "        return 'I am eating'\n",
    "    \n",
    "    def meow(self):\n",
    "        return 'meow meow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animal(object):\n",
    "    def eat(self):\n",
    "        return 'I am eating'\n",
    "\n",
    "class Dog(Animal):\n",
    "    \n",
    "    def woof(self):\n",
    "        return 'won won'\n",
    "\n",
    "class Cat(Animal):\n",
    "    def meow(self):\n",
    "        return 'meow meow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StemmedTfIDFVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = StemmedTfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = 'imaging database'\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff. 1.4142135623730951\n",
      "Imaging databases can get huge. 0.8681697052899789\n",
      "Most imaging databases safe images permanently. 0.859044512133176\n",
      "Imaging databases store images. 0.634205801303706\n",
      "Imaging databases store images. Imaging databases store images. Imaging databases store images. 0.634205801303706\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    d = dist(X[i], new_post_vec)\n",
    "    print(contents[i], d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文詞頻矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "jieba.load_userdict('userdict.txt')\n",
    "\n",
    "contents = ['柯文哲為了大巨蛋一事找趙藤雄算帳', \n",
    "     '柯P將不在大巨蛋舉辦世運會',\n",
    "     '今天天氣真好']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for rec in contents:\n",
    "    s = ' '.join(jieba.cut(rec))\n",
    "    corpus.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['柯文哲 為了 大巨蛋 一 事 找 趙藤雄 算帳', '柯P 將 不 在 大巨蛋 舉辦 世運會', '今天 天氣 真好']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x11 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['世運會', '今天', '大巨蛋', '天氣', '柯p', '柯文哲', '為了', '真好', '算帳', '舉辦', '趙藤雄']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "ed = euclidean_distances(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 2.64575131, 2.82842712],\n",
       "       [2.64575131, 0.        , 2.64575131],\n",
       "       [2.82842712, 2.64575131, 0.        ]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "cs = cosine_distances(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.       , 0.7763932, 1.       ],\n",
       "       [0.7763932, 0.       , 1.       ],\n",
       "       [1.       , 1.       , 0.       ]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cs = cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.       , 0.2236068, 0.       ],\n",
       "       [0.2236068, 1.       , 0.       ],\n",
       "       [0.       , 0.       , 1.       ]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 維基百科爬蟲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "res = requests.get('https://zh.wikipedia.org/wiki/%E6%9F%AF%E6%96%87%E5%93%B2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(res.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "s  = '/'.join([b.text for b in soup.select('.mw-parser-output p')[6].select('b')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'柯文哲/柯P/KP'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('synonym.txt', 'w') as f:\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'柯p': '柯文哲', 'kp': '柯文哲'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonym_dic = {}\n",
    "for s in open('synonym.txt'):\n",
    "    synonym = s.strip().split('/')\n",
    "    for w in synonym[1:]:\n",
    "        synonym_dic[w.lower()]  = synonym[0]\n",
    "synonym_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['世運會', '今天', '大巨蛋', '天氣', '柯文哲', '為了', '真好', '算帳', '舉辦', '趙藤雄']\n"
     ]
    }
   ],
   "source": [
    "import nltk.stem\n",
    "\n",
    "class SynonymCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(SynonymCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (synonym_dic.get(w, w) for w in analyzer(doc))\n",
    "\t\t\n",
    "vectorizer = SynonymCountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 1, 1, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "cs = cosine_distances(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.       , 0.5527864, 1.       ],\n",
       "       [0.5527864, 0.       , 1.       ],\n",
       "       [1.       , 1.       , 0.       ]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['為了', '一事', '不在']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['世運會', '今天', '大巨蛋', '天氣', '柯文哲', '真好', '算帳', '舉辦', '趙藤雄']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vectorizer = SynonymCountVectorizer(stop_words=stopwords)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0.5, 1. ],\n",
       "       [0.5, 0. , 1. ],\n",
       "       [1. , 1. , 0. ]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "cs = cosine_distances(X)\n",
    "cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新聞推薦引擎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "news = pandas.read_excel('https://raw.githubusercontent.com/ywchiu/obanktm/master/data/20171214news.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "titles = []\n",
    "for rec in news.iterrows():\n",
    "    titles.append(rec[1]['title'])\n",
    "    corpus.append(' '.join(jieba.cut(rec[1]['content'])))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<899x28942 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 122260 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "cs = cosine_distances(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(899, 899)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 0, 4, 2])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = numpy.array([80,70,90,65,88])\n",
    "a.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "反對水利會改制　吳敦義下令：藍委做好夜宿立院抗爭準備 0.6536245242470944\n",
      "農田水利會改公務機關　蔡英文：這不是綁樁 0.6791340621697286\n",
      "國防部解約慶富要提告　馮世寬怒嗆三遍：請便 0.7412839482000559\n",
      "罷免案將投票　李遠哲今再度現身力挺黃國昌 0.7452540222665425\n"
     ]
    }
   ],
   "source": [
    "for idx  in cs[0].argsort()[1:10]:\n",
    "    if cs[0][idx] < 0.75:\n",
    "        print(titles[idx], cs[0][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimiliarArticle(pos):\n",
    "    print('查詢文章：', titles[pos])\n",
    "    for idx  in cs[pos].argsort()[1:10]:\n",
    "        if cs[pos][idx] < 0.75:\n",
    "            print('相關文章：', titles[idx], cs[pos][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查詢文章： 黃國昌：長期目標要消滅國民黨　不支持柯文哲的兩岸一家親\n",
      "相關文章： 黃國昌若被罷免會更強　他：成為選輸北市的阿扁 0.5519166829972268\n",
      "相關文章： 罷免案將投票　李遠哲今再度現身力挺黃國昌 0.5752596978859429\n",
      "相關文章： 「兩蔣時代」超譯　網友：不准傷皇城內的和氣！ 0.5938224838071139\n",
      "相關文章： 罷昌案周六投票　時代力量全力澄清不實謠言 0.6402804718905146\n",
      "相關文章： 【聲援片】沈發惠呼籲罷昌案投「不同意」　黃國昌：謝謝您 0.7003246342755608\n"
     ]
    }
   ],
   "source": [
    "getSimiliarArticle(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  檢索文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '我要辦信用卡'\n",
    "query_vec = ' '.join(jieba.cut(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "q =  vectorizer.transform([query_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x28942 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = cosine_distances(q, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 899)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "780"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.argsort().flatten()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'行動支付破百億元'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[780]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimiliarArticleByTerm(query):\n",
    "    query_vec = ' '.join(jieba.cut(query))\n",
    "    q =  vectorizer.transform([query_vec])\n",
    "    cs = cosine_distances(q, X)\n",
    "    #print(cs.shape)\n",
    "    #print(cs.argsort())\n",
    "    for idx  in cs.argsort().flatten()[0:10]:\n",
    "        if cs[0][idx] < 0.9:\n",
    "            print(titles[idx], cs[0][idx])\n",
    "        #if cs[pos][idx] < 0.75:\n",
    "        #print('相關文章：', titles[idx], cs[pos][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "被控對金管會施壓　高銀：曾銘宗抹黑擬提告 0.7486876550249827\n",
      "數位化衝擊　張兆順：外銀認為台灣最適分行數60家 0.7566038082443822\n",
      "遠東商銀到底違反什麼法？金管會恐有未依法裁罰之虞 0.8028192483187253\n",
      "北韓高仿假鈔流入南韓　專家驚：近乎完美 0.8071526960400325\n",
      "黃國昌爆高雄銀替慶富不實增資　公司：非事實 0.830678634630351\n",
      "Apple等三大Pay　綁卡數達223萬張 0.8560368498502611\n",
      "曾銘宗爆「好大的高雄銀」　高雄銀嗆：保留法律追訴權 0.8875196823335381\n"
     ]
    }
   ],
   "source": [
    "getSimiliarArticleByTerm('銀行')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抓取常見問題集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "res = requests.get('https://www.o-bank.com/retail/event/event-faq')\n",
    "soup = BeautifulSoup(res.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "ary = []\n",
    "for rec in soup.select('.select_body'):\n",
    "    if rec.select_one('.content'):\n",
    "        question = rec.select_one('h4').text\n",
    "        answer = rec.select_one('.content').text\n",
    "        ary.append({'question':question, 'answer': answer.strip()})\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "df = pandas.DataFrame(ary)\n",
    "df.to_excel('obank_qa.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請各位寫個相似度比對引擎\n",
    "當我輸入問題：請問我要如何得到傳說對決虛寶?\n",
    "機器人是否能根據已知QA (obank_qa.xlsx)告知我相似度最高之對應答案?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>不需要\\r\\n\\t                                    \\...</td>\n",
       "      <td>如果移除並重新安裝O-Bank行動銀行APP，是否需重新啟用「設備綁定/生物辨識」服務?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>隨時隨地享受金融服務\\r\\n\\t                              ...</td>\n",
       "      <td>用網路銀行有甚麼好處？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>依據交易類別，透過網銀密碼或簡訊OTP驗證即可轉帳\\r\\n\\t               ...</td>\n",
       "      <td>如何使用網路/行動銀行進行轉帳？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>需使用智慧型手機/行動裝置\\n\\n您的智慧型手機/行動裝置必須具備GPS功能，且限在本行行動...</td>\n",
       "      <td>使用搖一搖進行收付款，有設備的限制嗎?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>請至網路/行動銀行：個人設定/轉帳相關設定/手機號碼就是帳號，進行設定。\\n\\n設定完成後，...</td>\n",
       "      <td>如何設定「手機號碼就是帳號」功能？</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              answer  \\\n",
       "0  不需要\\r\\n\\t                                    \\...   \n",
       "1  隨時隨地享受金融服務\\r\\n\\t                              ...   \n",
       "2  依據交易類別，透過網銀密碼或簡訊OTP驗證即可轉帳\\r\\n\\t               ...   \n",
       "3  需使用智慧型手機/行動裝置\\n\\n您的智慧型手機/行動裝置必須具備GPS功能，且限在本行行動...   \n",
       "4  請至網路/行動銀行：個人設定/轉帳相關設定/手機號碼就是帳號，進行設定。\\n\\n設定完成後，...   \n",
       "\n",
       "                                       question  \n",
       "0  如果移除並重新安裝O-Bank行動銀行APP，是否需重新啟用「設備綁定/生物辨識」服務?  \n",
       "1                                   用網路銀行有甚麼好處？  \n",
       "2                              如何使用網路/行動銀行進行轉帳？  \n",
       "3                           使用搖一搖進行收付款，有設備的限制嗎?  \n",
       "4                             如何設定「手機號碼就是帳號」功能？  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "qa = pandas.read_excel('/Users/davidchiu/course/obanktm/obank_qa.xlsx', index_col=0)\n",
    "#qa = pandas.read_excel('obank_qa.xlsx', index_col=0)\n",
    "qa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "questions = []\n",
    "answers = []\n",
    "for rec in qa.iterrows():\n",
    "    q = ' '.join(jieba.cut(rec[1]['question']))\n",
    "    a = rec[1]['answer']\n",
    "    corpus.append(q)\n",
    "    answers.append(a)\n",
    "    questions.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimiliarAnswerByTerm(query):\n",
    "    query_vec = ' '.join(jieba.cut(query))\n",
    "    #print(query_vec)\n",
    "    q =  vectorizer.transform([query_vec])\n",
    "    cs = cosine_distances(q, X)\n",
    "    idx = cs.argsort().flatten()[0]\n",
    "    print(questions[idx])\n",
    "    print(answers[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「 傳說 對決 聯名 卡 」 新 戶 活動 「 虛寶 」 如何 獲得 ?\n",
      "於2018/1/18起至2019/1/18前完成開戶且申辦「傳說對決聯名卡」的新戶，於開戶日後30天內完成「刷卡消費累積NT500元」或「一卡通自動加值NT500元」則符合獲得虛寶四選一的資格。\n",
      "完成刷卡或一卡通自動值累積NT500元交易後「3個營業日」(不含假日)內，發送虛寶兌獎簡訊通知。 \n",
      "完成「虛寶四選一」的選擇後「3個營業日」(不含假日)內，發送虛寶兌換序號簡訊通知，再至「Garena傳說對決遊戲」裡兌換。\n",
      "    (進入《Garena 傳說對決》遊戲－>點選禮物盒圖像－>遊戲公告－>活動序號兌換處－>去完成－>輸入序號－>兌換)\n",
      "任何序號兌換問題，請洽Garena客服中心詢問。\n",
      "完整「傳說對決聯名卡」請詳活動說明\n"
     ]
    }
   ],
   "source": [
    "getSimiliarAnswerByTerm('請問我要如何得到傳說對決虛寶? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 要 怎麼 查詢 O - Bank 存款 帳號 ?\n",
      "金融卡背面或登入O-Bank行動銀行\r\n",
      "\t                                    \n",
      "\n",
      "\n",
      "1.金融卡背面印有O-Bank存款帳戶之帳號（請留意! 金融卡正面為卡號，不是存款帳號喔）。\n",
      "\n",
      "2.登入O-Bank行動銀行後查詢「我的帳戶」\n"
     ]
    }
   ],
   "source": [
    "getSimiliarAnswerByTerm('如何查詢存款帳號? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
